{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T09:02:48.516972Z",
     "start_time": "2025-01-15T09:02:13.336473Z"
    }
   },
   "source": "%pip install faiss-cpu scikit-learn scipy",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting faiss-cpu\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/95/4b2f08400ab7509c989a288abf85fe93215b9da3e236881f22f975d5212b/faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl (13.8 MB)\n",
      "     ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/13.8 MB 10.0 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.8/13.8 MB 8.4 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/13.8 MB 5.4 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.2/13.8 MB 5.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.0/13.8 MB 4.9 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.5/13.8 MB 4.7 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.3/13.8 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.1/13.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.9/13.8 MB 4.2 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.7/13.8 MB 4.1 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 9.4/13.8 MB 4.1 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 10.2/13.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.7/13.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.8/13.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 12.3/13.8 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 13.4/13.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 13.8/13.8 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/27/585859e72e117fe861c2079bcba35591a84f801e21bc1ab85bce6ce60305/scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/11.1 MB 4.2 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.1/11.1 MB 5.6 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.1/11.1 MB 5.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 3.7/11.1 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 4.7/11.1 MB 4.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 5.5/11.1 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.3/11.1 MB 4.2 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 6.8/11.1 MB 4.2 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 7.6/11.1 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 8.1/11.1 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.9/11.1 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.7/11.1 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.5/11.1 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 11.1/11.1 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting scipy\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ff/ba/31c7a8131152822b3a2cdeba76398ffb404d81d640de98287d236da90c49/scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "     ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.3/43.6 MB 7.5 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 2.6/43.6 MB 7.6 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 3.1/43.6 MB 6.0 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 4.2/43.6 MB 5.0 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 4.7/43.6 MB 4.9 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 5.5/43.6 MB 4.6 MB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 6.3/43.6 MB 4.4 MB/s eta 0:00:09\n",
      "     ------ --------------------------------- 7.1/43.6 MB 4.3 MB/s eta 0:00:09\n",
      "     ------- -------------------------------- 7.9/43.6 MB 4.2 MB/s eta 0:00:09\n",
      "     ------- -------------------------------- 8.7/43.6 MB 4.1 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 9.2/43.6 MB 4.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 10.0/43.6 MB 4.0 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 11.0/43.6 MB 4.0 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 11.5/43.6 MB 4.0 MB/s eta 0:00:09\n",
      "     ----------- ---------------------------- 12.6/43.6 MB 3.9 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 13.4/43.6 MB 4.0 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 14.2/43.6 MB 3.9 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 14.9/43.6 MB 3.9 MB/s eta 0:00:08\n",
      "     -------------- ------------------------- 15.7/43.6 MB 3.9 MB/s eta 0:00:08\n",
      "     --------------- ------------------------ 16.5/43.6 MB 3.9 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 17.0/43.6 MB 3.9 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 18.1/43.6 MB 3.9 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 18.9/43.6 MB 3.9 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 19.7/43.6 MB 3.9 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 20.2/43.6 MB 3.8 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 21.0/43.6 MB 3.8 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 21.5/43.6 MB 3.8 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 22.3/43.6 MB 3.8 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 23.3/43.6 MB 3.8 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 23.6/43.6 MB 3.8 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 23.9/43.6 MB 3.7 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 24.1/43.6 MB 3.6 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 25.2/43.6 MB 3.6 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 25.4/43.6 MB 3.6 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 26.7/43.6 MB 3.6 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 28.0/43.6 MB 3.7 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 29.4/43.6 MB 3.8 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 29.9/43.6 MB 3.8 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 30.9/43.6 MB 3.8 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 31.7/43.6 MB 3.8 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 32.5/43.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 33.0/43.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 33.8/43.6 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 34.6/43.6 MB 3.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 35.4/43.6 MB 3.7 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 36.2/43.6 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 37.0/43.6 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 37.5/43.6 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 38.5/43.6 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 39.3/43.6 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 40.1/43.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 40.9/43.6 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.7/43.6 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 42.5/43.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  43.3/43.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  43.5/43.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  43.5/43.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 43.6/43.6 MB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in f:\\pycharm_project\\wow-agent\\.venv\\lib\\site-packages (from faiss-cpu) (2.2.1)\n",
      "Requirement already satisfied: packaging in f:\\pycharm_project\\wow-agent\\.venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in f:\\pycharm_project\\wow-agent\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, faiss-cpu, scikit-learn\n",
      "Successfully installed faiss-cpu-1.9.0.post1 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T02:55:07.769892Z",
     "start_time": "2025-01-16T02:55:07.752721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('ZISHU_API_KEY')\n",
    "#print(api_key)\n",
    "base_url = \"https://open.bigmodel.cn/api/paas/v4/\"\n",
    "chat_model = \"glm-4-flash\"\n",
    "emb_model = \"embedding-3\""
   ],
   "id": "df1e03b69ba8aa2f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T09:16:22.703465Z",
     "start_time": "2025-01-15T09:16:20.276749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key = api_key,\n",
    "    base_url = base_url\n",
    ")"
   ],
   "id": "f6c1bdc4c4c57558",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构造文档",
   "id": "d9bbbbe7feaf2c12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T09:16:49.854987Z",
     "start_time": "2025-01-15T09:16:49.850190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造文档（使用没有任何优化的顺序切分器，将文章分成了 150 个字符一段的小文本块。）\n",
    "\n",
    "embedding_text = \"\"\"\n",
    "Multimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models could help drive content generation for bots and AI agents, and assist in productivity applications, helping to re-play, paraphrase, action prediction or synthesize 3D or 2D scenario. Fundamental advances in agent AI help contribute towards these goals and many would benefit from a greater understanding of how to model embodied and empathetic in a simulate reality or a real world. Arguably many of these applications could have positive benefits.\n",
    "\n",
    "However, this technology could also be used by bad actors. Agent AI systems that generate content can be used to manipulate or deceive people. Therefore, it is very important that this technology is developed in accordance with responsible AI guidelines. For example, explicitly communicating to users that content is generated by an AI system and providing the user with controls in order to customize such a system. It is possible the Agent AI could be used to develop new methods to detect manipulative content - partly because it is rich with hallucination performance of large foundation model - and thus help address another real world problem.\n",
    "\n",
    "For examples, 1) in health topic, ethical deployment of LLM and VLM agents, especially in sensitive domains like healthcare, is paramount. AI agents trained on biased data could potentially worsen health disparities by providing inaccurate diagnoses for underrepresented groups. Moreover, the handling of sensitive patient data by AI agents raises significant privacy and confidentiality concerns. 2) In the gaming industry, AI agents could transform the role of developers, shifting their focus from scripting non-player characters to refining agent learning processes. Similarly, adaptive robotic systems could redefine manufacturing roles, necessitating new skill sets rather than replacing human workers. Navigating these transitions responsibly is vital to minimize potential socio-economic disruptions.\n",
    "\n",
    "Furthermore, the agent AI focuses on learning collaboration policy in simulation and there is some risk if directly applying the policy to the real world due to the distribution shift. Robust testing and continual safety monitoring mechanisms should be put in place to minimize risks of unpredictable behaviors in real-world scenarios. Our “VideoAnalytica\" dataset is collected from the Internet and considering which is not a fully representative source, so we already go through-ed the ethical review and legal process from both Microsoft and University Washington. Be that as it may, we also need to understand biases that might exist in this corpus. Data distributions can be characterized in many ways. In this workshop, we have captured how the agent level distribution in our dataset is different from other existing datasets. However, there is much more than could be included in a single dataset or workshop. We would argue that there is a need for more approaches or discussion linked to real tasks or topics and that by making these data or system available.\n",
    "\n",
    "We will dedicate a segment of our project to discussing these ethical issues, exploring potential mitigation strategies, and deploying a responsible multi-modal AI agent. We hope to help more researchers answer these questions together via this paper.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 设置每个文本块的大小为 150 个字符\n",
    "chunk_size = 150\n",
    "# 使用列表推导式将长文本分割成多个块，每个块的大小为 chunk_size\n",
    "chunks = [embedding_text[i:i + chunk_size] for i in range(0, len(embedding_text), chunk_size)]"
   ],
   "id": "b01a212a00936695",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 向量化（23块文本转化成了23个向量）",
   "id": "17f8571dfd2ad809"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:31:56.083983Z",
     "start_time": "2025-01-15T13:31:45.378857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 向量化（23块文本转化成了23个向量）\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# 初始化一个空列表来存储每个文本块的嵌入向量\n",
    "embeddings = []\n",
    "\n",
    "# 遍历每个文本块\n",
    "for chunk in chunks:\n",
    "    # 使用 OpenAI API 为当前文本块创建嵌入向量\n",
    "    response = client.embeddings.create(\n",
    "        model=emb_model,\n",
    "        input=chunk,\n",
    "    )\n",
    "\n",
    "    # 将嵌入向量添加到列表中\n",
    "    embeddings.append(response.data[0].embedding)\n",
    "\n",
    "# 使用 sklearn 的 normalize 函数对嵌入向量进行归一化处理\n",
    "normalized_embeddings = normalize(np.array(embeddings).astype('float32'))\n",
    "\n",
    "# 获取嵌入向量的维度\n",
    "d = len(embeddings[0])\n",
    "\n",
    "# 创建一个 Faiss 索引，用于存储和检索嵌入向量\n",
    "index = faiss.IndexFlatIP(d)\n",
    "\n",
    "# 将归一化后的嵌入向量添加到索引中\n",
    "index.add(normalized_embeddings)\n",
    "\n",
    "# 获取索引中的向量总数\n",
    "n_vectors = index.ntotal\n",
    "\n",
    "\n",
    "print(n_vectors)"
   ],
   "id": "d40fd83f2428af25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 给输入的文本匹配最合适的向量",
   "id": "8f59c05ad445d68a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:52:22.399162Z",
     "start_time": "2025-01-15T13:52:22.379704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def match_text(input_text, index, chunks, k=2):\n",
    "    \"\"\"\n",
    "    在给定的文本块集合中，找到与输入文本最相似的前k个文本块。\n",
    "\n",
    "    参数:\n",
    "        input_text (str): 要匹配的输入文本。\n",
    "        index (faiss.Index): 用于搜索的Faiss索引。\n",
    "        chunks (list of str): 文本块的列表。\n",
    "        k (int, optional): 要返回的最相似文本块的数量。默认值为2。\n",
    "\n",
    "    返回:\n",
    "        str: 格式化的字符串，包含最相似的文本块及其相似度。\n",
    "    \"\"\"\n",
    "    # 确保k不超过文本块的总数\n",
    "    k = min(k, len(chunks))\n",
    "\n",
    "    # 使用OpenAI API为输入文本创建嵌入向量\n",
    "    response = client.embeddings.create(\n",
    "        model=emb_model,\n",
    "        input=input_text,\n",
    "    )\n",
    "    # 获取输入文本的嵌入向量\n",
    "    input_embedding = response.data[0].embedding\n",
    "    # 对输入嵌入向量进行归一化处理\n",
    "    input_embedding = normalize(np.array([input_embedding]).astype('float32'))\n",
    "\n",
    "    # 在索引中搜索与输入嵌入向量最相似的k个向量\n",
    "    distances, indices = index.search(input_embedding, k)\n",
    "    # 初始化一个字符串来存储匹配的文本\n",
    "    matching_texts = \"\"\n",
    "    # 遍历搜索结果\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        # 打印每个匹配文本块的相似度和文本内容\n",
    "        print(f\"similarity: {distances[0][i]:.4f}\\nmatching text: \\n{chunks[idx]}\\n\")\n",
    "        # 将相似度和文本内容添加到匹配文本字符串中\n",
    "        matching_texts += f\"similarity: {distances[0][i]:.4f}\\nmatching text: \\n{chunks[idx]}\\n\"\n",
    "\n",
    "    # 返回包含匹配文本块及其相似度的字符串\n",
    "    return matching_texts\n"
   ],
   "id": "4598116e47713d0e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 尝试检索",
   "id": "aef684095cf1354f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:52:25.007995Z",
     "start_time": "2025-01-15T13:52:24.564657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"What are the applications of Agent AI systems ?\"\n",
    "\n",
    "matched_texts = match_text(input_text=input_text, index=index, chunks=chunks, k=2)"
   ],
   "id": "866c69fd158bfc06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.6366\n",
      "matching text: \n",
      "ystem and providing the user with controls in order to customize such a system. It is possible the Agent AI could be used to develop new methods to de\n",
      "\n",
      "similarity: 0.5774\n",
      "matching text: \n",
      "\n",
      "Multimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models could help drive content generation for\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建对话引擎",
   "id": "8303a82e27e5a899"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:59:09.465902Z",
     "start_time": "2025-01-15T13:59:09.460886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = f\"\"\"\n",
    "根据找到的文档\n",
    "{matched_texts}\n",
    "生成\n",
    "{input_text}\n",
    "的答案，尽可能使用文档语句的原文回答。不要复述问题，直接开始回答。\n",
    "\"\"\"\n",
    "\n",
    "def get_completion_stream(prompt):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的 Chat Completions API 生成流式的文本回复。\n",
    "\n",
    "    参数:\n",
    "        prompt (str): 要生成回复的提示文本。\n",
    "\n",
    "    返回:\n",
    "        None: 该函数直接打印生成的回复内容。\n",
    "    \"\"\"\n",
    "    # 使用 OpenAI 的 Chat Completions API 创建一个聊天完成请求\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_model,  # 填写需要调用的模型名称\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    # 如果响应存在\n",
    "    if response:\n",
    "        # 遍历响应中的每个块\n",
    "        for chunk in response:\n",
    "            # 获取当前块的内容\n",
    "            content = chunk.choices[0].delta.content\n",
    "            # 如果内容存在\n",
    "            if content:\n",
    "                # 打印内容，并刷新输出缓冲区\n",
    "                print(content, end='', flush=True)\n"
   ],
   "id": "983dfdbcf48b5b2a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:59:14.849008Z",
     "start_time": "2025-01-15T13:59:12.673353Z"
    }
   },
   "cell_type": "code",
   "source": "get_completion_stream(prompt)",
   "id": "aa1a6def7188abbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models could help drive content generation for various purposes. The systems and providing the user with controls in order to customize such a system are also possible. It is possible that Agent AI could be used to develop new methods to de."
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
